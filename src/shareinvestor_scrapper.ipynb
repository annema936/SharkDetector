{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\fastai\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning:\n",
      "\n",
      "The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from session_handler import *\n",
    "import json\n",
    "from datetime import datetime,date,timezone\n",
    "import pytz\n",
    "import urllib.parse\n",
    "from klse_scrapper import *\n",
    "from utils import *\n",
    "import os\n",
    "from workalendar.asia import Malaysia\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgtz=pytz.timezone('Asia/Singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_time_sales(session,event):\n",
    "    date_=datetime.fromtimestamp(event['to']).strftime(\"%Y%m%d\")\n",
    "    URL = 'http://www.shareinvestor.com/prices/time_and_sales_f.html'\n",
    "    params={'date':date_,'counter':event['counter']+'.MY','page':-1}\n",
    "    params=urllib.parse.urlencode(params)\n",
    "    URL=URL+'?'+params\n",
    "    page = session.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    rows=soup.find('table',{'id':\"sic_timeAndSalesTable\"}).findAll('tr')\n",
    "    lists=[]\n",
    "    for i in range(2,len(rows)):\n",
    "        raw_vals=rows[i].findAll('td')\n",
    "        time_=raw_vals[0].text\n",
    "        type_=raw_vals[1].text\n",
    "        last_done_=float(raw_vals[2].text)\n",
    "        price_chg_=raw_vals[3].text\n",
    "        vol_chg_=int(raw_vals[4].text.replace(',',''))\n",
    "        vol_total_=int(raw_vals[5].text.replace(',',''))\n",
    "        row={'time':date_+' '+time_,'type':type_,'last_done':last_done_,'price_chg':price_chg_,'vol_chg':vol_chg_,'vol_total':vol_total_}\n",
    "        lists.append(row)\n",
    "    df=pd.DataFrame(lists).set_index('time')\n",
    "    df.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_quote_movements(session,event,live=False):\n",
    "    date_=datetime.fromtimestamp(event['to'])\n",
    "    date_=date_.strftime(\"%Y-%m-%d\")\n",
    "    empty_chars='\\xa0'     \n",
    "    URL = 'http://www.shareinvestor.com/prices/quote_movements_f.html'\n",
    "    if live:\n",
    "        pageno=1\n",
    "    else:\n",
    "        pageno=-1\n",
    "    params={'date':date_,'counter':event['counter']+'.MY','page':pageno}\n",
    "    params=urllib.parse.urlencode(params)\n",
    "    URL=URL+'?'+params\n",
    "    page = session.get(URL)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    rows=soup.find('table',{'id':\"sic_quoteMovementTable\"}).findAll('tr')\n",
    "    lists=[]\n",
    "    for i in range(2,len(rows)):\n",
    "        raw_vals=rows[i].findAll('td')\n",
    "        if(len(raw_vals)>=7):\n",
    "            time_=raw_vals[0].text\n",
    "            bq_vol_ = None if raw_vals[1].text==empty_chars else raw_vals[1].text\n",
    "            try:\n",
    "                bq_price_=float(raw_vals[2].text)\n",
    "            except Exception as e:\n",
    "                bq_price_=None\n",
    "            ld_vol=None if raw_vals[3].text==empty_chars else float(raw_vals[3].text.replace(',',''))\n",
    "            try:\n",
    "                ld_price=float(raw_vals[4].text)\n",
    "            except Exception as e: \n",
    "                ld_price=None\n",
    "            type_=None if raw_vals[5].text==empty_chars else raw_vals[5].text\n",
    "            sq_vol_=None if raw_vals[6].text==empty_chars else raw_vals[6].text\n",
    "            try:\n",
    "                sq_price_=float(raw_vals[7].text)\n",
    "            except Exception as e:\n",
    "                sq_price_=None\n",
    "            row={'time':date_+' '+time_,'buy_queue_vol':bq_vol_,'buy_queue_price':bq_price_,'last_done_vol':ld_vol,'last_done_price':ld_price,'type':type_,'sell_queue_vol':sq_vol_,'sell_queue_price':sq_price_}\n",
    "            lists.append(row)\n",
    "    df=pd.DataFrame(lists).set_index('time')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_quote_movements(session,event):\n",
    "    df_=crawl_quote_movements(session,event,True)\n",
    "    date_=event['to']\n",
    "    df_=df_.reset_index()\n",
    "    df_['time']=pd.to_datetime(df_['time'])\n",
    "    for i,row in df_.iterrows():\n",
    "        if(row['time'].replace(tzinfo=sgtz).timestamp()<date_):\n",
    "            break    \n",
    "    return df_[0:i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_quote_movements(session,board,category,from_,to_):\n",
    "    df=get_board_category_listings(board,category)\n",
    "    lists_=df[['symbol','code','cat']].to_dict('records')\n",
    "    events=build_event_lists(lists_=lists_,from_=from_,to_=to_)\n",
    "    for event in events:\n",
    "        events_=distribute_requests(event)\n",
    "        for event_ in events_:\n",
    "            try:\n",
    "                cal = Malaysia()\n",
    "                date_=datetime.fromtimestamp(event_['to'])\n",
    "                if(cal.is_working_day(date(date_.year,date_.month,date_.day))):\n",
    "                    date_=date_.strftime(\"%Y-%m-%d\")\n",
    "                    dir_='history/quote_movements/'\n",
    "                    dir_=os.path.join(dir_,event_['category'])\n",
    "                    name=event_['symbol']+'_'+event_['counter']\n",
    "                    dir_=os.path.join(dir_,name)\n",
    "                    if not os.path.exists(dir_): os.makedirs(dir_)\n",
    "                    save_path=os.path.join(dir_,name)+'_'+date_+'.csv'\n",
    "                    #if csv exists, return dataframe from existing csv\n",
    "                    if os.path.exists(save_path): \n",
    "                        print('CSV exists. Not going to crawl.')\n",
    "                    else:\n",
    "                        df=crawl_quote_movements(session,event_)\n",
    "                        df.to_csv(save_path)\n",
    "            except Exception as e: print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'live_quote_movements' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'live_quote_movements' is not defined"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_buy_sell_queue' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7508c1c5c0d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mevent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'to'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1591952280\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'counter'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'2127'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'df=live_quote_movements(session,event)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_buy_sell_queue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'last_done_price'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'last'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_buy_sell_queue' is not defined"
     ]
    }
   ],
   "source": [
    "event={'to':1591952280,'counter':'2127'}\n",
    "%time df=live_quote_movements(session,event)\n",
    "df = split_buy_sell_queue(df)\n",
    "df=df.iloc[::-1]\n",
    "df.groupby('type').agg({'last_done_price':'last'})\n",
    "df.groupby('sell_queue_price').agg({'sell_queue_vol':'last','sell_vol_chg':'sum'})\n",
    "df.groupby('buy_queue_price').agg({'buy_queue_vol':'last','buy_vol_chg':'sum'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "\"None of ['time'] are in the columns\"\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "CSV exists. Not going to crawl.\n",
      "Wall time: 48min 17s\n"
     ]
    }
   ],
   "source": [
    "generate_session()\n",
    "session=read_session()\n",
    "%time mine_quote_movements(session=session,board='Main Market',category='Energy',from_=1585702800,to_=1588986000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Industrial Products & Services\n"
     ]
    }
   ],
   "source": [
    "dufu=Counter('DUFU','7183',scheduler_on=False)\n",
    "#dufu.refresh_volume()\n",
    "dufu.get_day_volume()\n",
    "dufu.detect_shark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
